{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b7287c-88c2-494d-9aa5-300461a2253e",
   "metadata": {},
   "source": [
    "# Characteristics Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1c950-76d2-436e-b59f-4d84bffac7da",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a846c4-3812-4bba-aeac-9c0c96a5c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import DiagnosisFunctions.tools as tools\n",
    "\n",
    "import albumentations as A\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6b8a5-6130-4cd8-95b6-f4a497c680b1",
   "metadata": {},
   "source": [
    "Get splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0dc611e-c209-43f5-b6a8-d7729bf88a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_path, train_target), (val_path, val_target), (test_path, test_target), le = tools.get_splits_characteristics()\n",
    "\n",
    "train_set    = tools.CharacteristicsDataset(path = train_path, target = train_target, size = [200, 200])\n",
    "train_loader = DataLoader(train_set, batch_size=16)\n",
    "\n",
    "val_set    = tools.CharacteristicsDataset(path = val_path, target = val_target, size = [200, 200])\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "\n",
    "test_set    = tools.CharacteristicsDataset(path = test_path, target = test_target, size = [200, 200])\n",
    "test_loader = DataLoader(test_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b4b9e-2d65-448b-9f7a-1a33291ab64c",
   "metadata": {},
   "source": [
    "# Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8934b182-9069-48d3-a02e-48a19f3a2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 8\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        resnet = models.resnext50_32x4d(pretrained=True)\n",
    "        resnet.fc = nn.Sequential(\n",
    "            #nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n",
    "        )\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943bd3e4-7c7f-44ba-8a31-bd5ba3a32860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360e109ec45a4169a3446bd640e16a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e196414e6779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(cnn.parameters())\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses     = []\n",
    "train_accuracies = []\n",
    "\n",
    "val_losses       = []\n",
    "val_accuracies   = []\n",
    "\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs), unit='epoch', desc=\"Epoch\"):\n",
    "\n",
    "    #Epochs start @ 1 now.\n",
    "    epoch += 1\n",
    "\n",
    "    ## -- Training -- ##\n",
    "    cnn.train()\n",
    "    train_loss = 0\n",
    "    predictions  = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for data in train_loader:\n",
    "\n",
    "        #Fetch images and targets from train loader.\n",
    "        images, targets = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn(images)\n",
    "        \n",
    "        loss = criterion(outputs, torch.FloatTensor(targets))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Add the batch loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #Save predictions and targets\n",
    "        predictions  += outputs.tolist()\n",
    "        ground_truth += targets.tolist()\n",
    "        \n",
    "    #Append this epoch's statistics.\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(accuracy_score(ground_truth, predictions))\n",
    "    \n",
    "    ## -- End of Training -- ##\n",
    "    \n",
    "    ## -- Validation -- ##\n",
    "    cnn.eval()\n",
    "    val_loss = 0\n",
    "    predictions  = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for data in val_loader:\n",
    "    \n",
    "        #Fetch images and targets from val loader.\n",
    "        images, targets = data\n",
    "        \n",
    "        #Get that stuff on the GPU\n",
    "        #images  = images.to(device)\n",
    "        #targets = targets.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn(images)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Add the batch loss\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        #Save predictions and targets\n",
    "        predictions  += outputs.tolist()\n",
    "        ground_truth += targets.tolist()\n",
    "        \n",
    "    #Append this epoch's statistics.\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(accuracy_score(ground_truth, predictions))\n",
    "    ## -- End of Validation -- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd4e5c6-d2f7-47f5-9087-8c2f58cc64fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1251f67-8247-4371-9469-9215461792ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765658fa-d5f1-4b18-a7ac-9c25b62b21f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
