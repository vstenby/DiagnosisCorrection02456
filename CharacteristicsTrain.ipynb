{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that predicts characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import DiagnosisFunctions.tools as tools\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations as A\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import CNNmodels as CNNmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is running on the cuda.\n",
      "Running on device 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the notebook to run on the GPU, if available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'This notebook is running on the {device.type}.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Running on device {torch.cuda.current_device()}\")\n",
    "    print('')\n",
    "    #Set the batch size on cuda\n",
    "    batch_size = 64\n",
    "\n",
    "else:\n",
    "    batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_path, train_target), (test_path, test_target) = tools.get_splits_characteristics()\n",
    "\n",
    "train_set = tools.CharacteristicsDataset(path = train_path, target = train_target, size = [200, 200])\n",
    "test_set = tools.CharacteristicsDataset(path = test_path, target = test_target, size = [200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target, characteristics = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(phase, model, optimizer, criterion, scheduler, dataloaders):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    #Preallocate the probabilities dataframe.\n",
    "    probabilities = pd.DataFrame(columns = dataloaders[phase].dataset.variables)\n",
    "    ground_truth  = pd.DataFrame(columns = dataloaders[phase].dataset.variables)\n",
    "\n",
    "    for inputs, targets, _ in dataloaders[phase]:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #Append to the dataframes\n",
    "        probabilities = probabilities.append(pd.DataFrame(outputs.detach().cpu().numpy(), columns = dataloaders[phase].dataset.variables), ignore_index=True)\n",
    "        ground_truth  = ground_truth.append(pd.DataFrame(targets.detach().cpu().numpy(), columns  = dataloaders[phase].dataset.variables), ignore_index=True)\n",
    "\n",
    "    if phase == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "    #Return the total loss.\n",
    "    return running_loss, ground_truth, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_predictions(gt, p):\n",
    "    assert np.all(p.columns == gt.columns), 'Columns should be the same.'\n",
    "\n",
    "    #Calculate the diagnosis f1 score.\n",
    "    diagnosis_p = p[[x for x in p.columns if 'diagnosis_' in x]]\n",
    "    diagnosis_gt = gt[[x for x in gt.columns if 'diagnosis_' in x]]\n",
    "    assert np.all(diagnosis_p.columns == diagnosis_gt.columns), 'Columns should be the same'\n",
    "\n",
    "    #Find the diagnosis f1 macro.\n",
    "    diagnosis_p_pred  = diagnosis_p.values.argmax(axis=1)\n",
    "    diagnosis_gt_pred = diagnosis_gt.values.argmax(axis=1) \n",
    "    diagnosis_f1      = f1_score(diagnosis_gt_pred, diagnosis_p_pred, average='macro')\n",
    "\n",
    "    return diagnosis_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0: 100%|██████████| 10/10 [00:42<00:00,  4.27s/epoch]\n",
      "Fold 1: 100%|██████████| 10/10 [00:42<00:00,  4.27s/epoch]\n",
      "Fold 2: 100%|██████████| 10/10 [00:43<00:00,  4.33s/epoch]\n",
      "Fold 3: 100%|██████████| 10/10 [00:38<00:00,  3.89s/epoch]\n",
      "Fold 4: 100%|██████████| 10/10 [00:42<00:00,  4.20s/epoch]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/CharacteristicStats_2021-12-03 13:27:05.963211.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f8cda6fecdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#Save the results to a pickle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'results/CharacteristicStats_{datetime.now().__str__()}.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf1_diagnosis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_characteristics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_area\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/CharacteristicStats_2021-12-03 13:27:05.963211.p'"
     ]
    }
   ],
   "source": [
    "splits = KFold(n_splits=k)\n",
    "\n",
    "loss = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "f1_characteristics = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "f1_diagnosis = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "f1_area = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_set)))):\n",
    "    # Define train sampler and val sampler.\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler   = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader  = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, num_workers=6)\n",
    "    val_loader    = DataLoader(train_set, batch_size=batch_size, sampler=val_sampler, num_workers=6)\n",
    "\n",
    "    cnn = CNNmodels.CNN2(n_characteristics=7, n_diagnosis=6, n_area=4).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    #Update the dataloaders passed to the training function.\n",
    "    dataloaders = {'train' : train_loader, 'val' : val_loader}\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=f'Fold {fold}', unit='epoch'):\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_loss, gt, p = train_and_eval(phase, cnn, optimizer, criterion, scheduler, dataloaders)\n",
    "\n",
    "            if phase == 'train':\n",
    "                avg_obs_loss = (epoch_loss / len(train_idx)) \n",
    "            elif phase == 'val':\n",
    "                avg_obs_loss = (epoch_loss / len(val_idx))\n",
    "\n",
    "            loss[phase][fold].append(avg_obs_loss)\n",
    "\n",
    "            # Predict labels based on probabilities\n",
    "            pred_class = tools.classify_probability_predictions(p.copy())\n",
    "            \n",
    "            # Compute f1 scores with average 'samples' (default values)\n",
    "            metric_dict = tools.compute_metrics_scores(gt, pred_class)\n",
    "            \n",
    "            f1_characteristics[phase][fold].append(metric_dict['characteristics'])\n",
    "            f1_diagnosis[phase][fold].append(metric_dict['diagnosis'])\n",
    "            f1_area[phase][fold].append(metric_dict['area'])\n",
    "\n",
    "#Save the results to a pickle.\n",
    "with open(f'results/CharacteristicStats_{datetime.now().__str__()}.p', 'wb') as output_file:\n",
    "    pickle.dump([num_epochs, k, loss, (f1_diagnosis, f1_characteristics, f1_area)], output_file)\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    raise NotImplementedError(\"Let's stop the GPU here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characteristics': 0.7928145657510496,\n",
       " 'diagnosis': 0.6807947287573772,\n",
       " 'area': 0.8320903320903321}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d68afc4610b9f77056e36dfa169034149cc439201178febb46f41ce57e3ecf41"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
