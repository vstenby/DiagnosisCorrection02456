{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that predicts characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1,2\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import DiagnosisFunctions.tools as tools\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations as A\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import CNNmodels as CNNmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take 16\n",
      "This notebook is running on the cpu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Take 16')\n",
    "\n",
    "#Set the notebook to run on the GPU, if available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'This notebook is running on the {device.type}.')\n",
    "print('')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.set_device(1)\n",
    "\n",
    "    #Set the batch size on cuda\n",
    "    batch_size = 64\n",
    "\n",
    "else:\n",
    "    batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_path, train_target), (test_path, test_target) = tools.get_splits_characteristics()\n",
    "\n",
    "train_set    = tools.CharacteristicsDataset(path = train_path, target = train_target, size = [200, 200])\n",
    "test_set     = tools.CharacteristicsDataset(path = test_path, target = test_target,   size = [200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target, characteristics = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(phase, model, optimizer, criterion, scheduler, dataloaders):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    #Preallocate the probabilities dataframe.\n",
    "    probabilities = pd.DataFrame(columns = dataloaders[phase].dataset.variables)\n",
    "    ground_truth  = pd.DataFrame(columns = dataloaders[phase].dataset.variables)\n",
    "\n",
    "    for inputs, targets, _ in dataloaders[phase]:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #Append to the dataframes\n",
    "        probabilities = probabilities.append(pd.DataFrame(outputs.detach().cpu().numpy(), columns = dataloaders[phase].dataset.variables), ignore_index=True)\n",
    "        ground_truth  = ground_truth.append(pd.DataFrame(targets.detach().cpu().numpy(), columns  = dataloaders[phase].dataset.variables), ignore_index=True)\n",
    "\n",
    "    if phase == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "    #Return the total loss.\n",
    "    return running_loss, ground_truth, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0: 100%|██████████| 1/1 [00:45<00:00, 45.62s/epoch]\n",
      "Fold 1: 100%|██████████| 1/1 [00:45<00:00, 45.94s/epoch]\n",
      "Fold 2: 100%|██████████| 1/1 [00:46<00:00, 46.37s/epoch]\n"
     ]
    }
   ],
   "source": [
    "splits = KFold(n_splits=k)\n",
    "\n",
    "loss = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "f1_characteristics = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "f1_diagnosis = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "f1_area = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_set)))):\n",
    "    \n",
    "    # Define train sampler and val sampler.\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler   = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader  = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader    = DataLoader(train_set, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "    cnn = CNNmodels.CNN2(n_characteristics = 7, n_diagnosis = 6, n_area = 4).to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    #Update the dataloaders passed to the training function.\n",
    "    dataloaders = {'train' : train_loader, 'val' : val_loader}\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=f'Fold {fold}', unit='epoch'):\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_loss, gt, p = train_and_eval(phase, cnn, optimizer, criterion, scheduler, dataloaders)\n",
    "\n",
    "            if phase == 'train':\n",
    "                avg_obs_loss = (epoch_loss / len(train_idx)) #.detach().cpu()\n",
    "            elif phase == 'val':\n",
    "                avg_obs_loss = (epoch_loss / len(val_idx)) #.detach().cpu()\n",
    "\n",
    "            loss[phase][fold].append(avg_obs_loss)\n",
    "\n",
    "            # Predict labels based on probabilities\n",
    "            pred_class = tools.classify_probability_predictions(p.copy())\n",
    "            \n",
    "            # Compute f1 scores with average 'samples' (default values)\n",
    "            characteristics_scores, diagnosis_scores, area_scores = tools.compute_metrics_scores(gt, pred_class)\n",
    "            f1_characteristics[phase][fold].append(characteristics_scores)\n",
    "            f1_diagnosis[phase][fold].append(diagnosis_scores)\n",
    "            f1_area[phase][fold].append(area_scores)\n",
    "\n",
    "#Save the results to a pickle.\n",
    "with open('statistics.p', 'wb') as output_file:\n",
    "    pickle.dump([num_epochs, k, loss, f1_characteristics, f1_diagnosis, f1_area], output_file)\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    raise NotImplementedError(\"Let's stop the GPU here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "file = open('statistics.p', 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "nepoch, nfolds, loss, f1_characteristics, f1_diagnosis, f1_area = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(range(nepoch), np.array(loss['train']).T.mean(axis=1), yerr=np.array(loss['train']).T.std(axis=1), capsize=4, capthick=2, label='Train')\n",
    "#plt.errorbar(range(nepoch), np.array(loss['val']).T.mean(axis=1),   yerr=np.array(loss['val']).T.std(axis=1), capsize=4, capthick=2, label='Validation')\n",
    "plt.legend()\n",
    "plt.xticks(range(0,nepoch), range(1,nepoch+1))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean observation loss over5-fold CV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Characteristics: f1-samples', 'Diagnosis: f1-samples', 'Area: f1-samples']\n",
    "scores = [f1_characteristics, f1_diagnosis, f1_area]\n",
    "\n",
    "figWidth = 26\n",
    "figHeight = 6\n",
    "nRow = 1\n",
    "nCol = len(titles)\n",
    "epochs = np.arange(0, num_epochs)\n",
    "\n",
    "fig, axes = plt.subplots(nRow, nCol, figsize = (figWidth, figHeight))\n",
    "for i in range(0,nRow*nCol):\n",
    "\n",
    "    r = i//nCol\n",
    "    c = i%nCol\n",
    "\n",
    "    # Plot mean training and validation score distributions\n",
    "    axes[c].plot(epochs, [mean(scores[r+c]['train'][i]) for i in epochs], label='Training score')\n",
    "    # axes[c].plot(epochs, [mean(scores[r+c]['val'][i]) for i in epochs], label='Validation score')\n",
    "\n",
    "    # Plot k-fold distribution\n",
    "    axes[c].boxplot(scores[r+c]['train'], positions=epochs)\n",
    "    \n",
    "    axes[c].set_title(titles[i])\n",
    "    axes[c].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc835b8f07864e08f7a971334794c2c9a38c607ac9f698564281553fe9803e3c"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
