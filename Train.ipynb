{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook\n",
    "\n",
    "This notebook contains code to train the extended network using a train and validation set to obtain optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import DiagnosisFunctions.tools as tools\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations as A\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import CNNmodels as CNNmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is running on the cpu.\n"
     ]
    }
   ],
   "source": [
    "#Set the notebook to run on the GPU, if available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'This notebook is running on the {device.type}.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Running on device {torch.cuda.current_device()}\")\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_path, train_target), (test_path, test_target) = tools.get_splits_characteristics()\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        #ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4, p=0.5),\n",
    "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.4, hue=0, always_apply=False, p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = tools.CharacteristicsDataset(path = train_path, target = train_target, size = [200, 200], transform = train_transform)\n",
    "test_set = tools.CharacteristicsDataset(path = test_path, target = test_target, size = [200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target, characteristics = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(phase, model, optimizer, criterion, scheduler, dataloaders):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    #Preallocate the probabilities dataframe.\n",
    "    probabilities = pd.DataFrame(columns = dataloaders[phase].dataset.variables)\n",
    "    ground_truth  = pd.DataFrame(columns = dataloaders[phase].dataset.variables)\n",
    "\n",
    "    for inputs, targets, _ in dataloaders[phase]:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #Append to the dataframes\n",
    "        probabilities = probabilities.append(pd.DataFrame(outputs.detach().cpu().numpy(), columns = dataloaders[phase].dataset.variables), ignore_index=True)\n",
    "        ground_truth  = ground_truth.append(pd.DataFrame(targets.detach().cpu().numpy(), columns  = dataloaders[phase].dataset.variables), ignore_index=True)\n",
    "\n",
    "    if phase == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "    #Return the total loss.\n",
    "    return running_loss, ground_truth, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_predictions(gt, p):\n",
    "    assert np.all(p.columns == gt.columns), 'Columns should be the same.'\n",
    "\n",
    "    #Calculate the diagnosis f1 score.\n",
    "    diagnosis_p = p[[x for x in p.columns if 'diagnosis_' in x]]\n",
    "    diagnosis_gt = gt[[x for x in gt.columns if 'diagnosis_' in x]]\n",
    "    assert np.all(diagnosis_p.columns == diagnosis_gt.columns), 'Columns should be the same'\n",
    "\n",
    "    #Find the diagnosis f1 macro.\n",
    "    diagnosis_p_pred  = diagnosis_p.values.argmax(axis=1)\n",
    "    diagnosis_gt_pred = diagnosis_gt.values.argmax(axis=1) \n",
    "    diagnosis_f1      = f1_score(diagnosis_gt_pred, diagnosis_p_pred, average='macro')\n",
    "\n",
    "    return diagnosis_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCELoss():\n",
    "    def __init__(self, weights=[1, 1, 1]):\n",
    "        self.weights = weights\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def __call__(self, probabilities, targets):\n",
    "        loss_characteristics = self.criterion(probabilities[:, :7], targets[:, :7]) \n",
    "        loss_diagnosis       = self.criterion(probabilities[:, 7:13], targets[:, 7:13]) \n",
    "        loss_area            = self.criterion(probabilities[:, 13:], targets[:, 13:])\n",
    "\n",
    "        return self.weights[0] * loss_characteristics + self.weights[1] * loss_diagnosis + self.weights[2] * loss_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # parameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 128, step=8)\n",
    "    weights = [trial.suggest_float(f\"weight_{type}\", 0, 1) for type in ['characteristics', 'diagnosis', 'area']]\n",
    "\n",
    "    # training code\n",
    "    splits = KFold(n_splits=k)\n",
    "\n",
    "    loss = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "    f1_characteristics = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "    f1_diagnosis = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "    f1_area = {'train': [[] for _ in range(k)], 'val': [[] for _ in range(k)]}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_set)))):\n",
    "        # Define train sampler and val sampler.\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler   = SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_loader  = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader    = DataLoader(train_set, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        cnn = CNNmodels.CNN(n_characteristics=7, n_diagnosis=6, n_area=4).to(device)\n",
    "        \n",
    "        criterion = WeightedBCELoss(weights=weights)\n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        #Update the dataloaders passed to the training function.\n",
    "        dataloaders = {'train' : train_loader, 'val' : val_loader}\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc=f'Fold {fold}', unit='epoch'):\n",
    "            for phase in ['train', 'val']:\n",
    "                epoch_loss, gt, p = train_and_eval(phase, cnn, optimizer, criterion, scheduler, dataloaders)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    avg_obs_loss = (epoch_loss / len(train_idx)) \n",
    "                elif phase == 'val':\n",
    "                    avg_obs_loss = (epoch_loss / len(val_idx))\n",
    "\n",
    "                loss[phase][fold].append(avg_obs_loss)\n",
    "\n",
    "                # Predict labels based on probabilities\n",
    "                pred_class = tools.classify_probability_predictions(p.copy())\n",
    "                \n",
    "                # Compute f1 scores with average 'samples' (default values)\n",
    "                metric_dict = tools.compute_metrics_scores(gt, pred_class)\n",
    "                \n",
    "                f1_characteristics[phase][fold].append(metric_dict['characteristics'])\n",
    "                f1_diagnosis[phase][fold].append(metric_dict['diagnosis'])\n",
    "                f1_area[phase][fold].append(metric_dict['area'])\n",
    "\n",
    "    #Save the results to a pickle.\n",
    "    with open(f'results/CharacteristicStats_{datetime.now().__str__()}.p', 'wb') as output_file:\n",
    "        pickle.dump([num_epochs, k, loss, (f1_diagnosis, f1_characteristics, f1_area)], output_file)\n",
    "\n",
    "    return np.mean(f1_diagnosis['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "print(\"Starting Optuna study\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "with open(f\"studies/{datetime.now().__str__()}.p\", 'wb') as output_file:\n",
    "    pickle.dump(study, output_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d68afc4610b9f77056e36dfa169034149cc439201178febb46f41ce57e3ecf41"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
